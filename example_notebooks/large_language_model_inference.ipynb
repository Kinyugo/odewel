{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Kinyugo/odewel/blob/main/example_notebooks/large_language_model_inference.ipynb)\n",
        "\n",
        "# 🏋️ Large Language Model Inference 💪\n",
        "\n",
        "This notebook illustrates how to perform inference using `odewel`. It showcases the power of `odewel` that enables one to run models on hardware with limited memory capacity.\n",
        "\n",
        "We demonstrate the usage of `odewel` with the Flan-T5-XXL model. Specifically, we utilize the sharded version created by Phil Schmid to avoid the requirement of a device with 80GB of memory for model preparation 🥶."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q git+https://github.com/Kinyugo/odewel.git\n",
        "%pip install -q transformers accelerate sentencepiece\n",
        "%pip install -q joblib"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from transformers import T5Config, T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "from odewel.torch import ShardedWeightsLoader, init_on_demand_weights_model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch Checkpoints \n",
        "\n",
        "We will download the checkpoints from Hugging Face (HF).\n",
        "\n",
        "> 💡 Although we are downloading the checkpoints here, the implementation of `odewel` is flexible enough to allow for downloading checkpoints during inference time. This could be useful in cases where storage is also limited, as you can download the required checkpoint just in time for inference, rather than having to store it on disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/philschmid/flan-t5-xxl-sharded-fp16"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Inference With On Demand Weights Loading\n",
        "\n",
        "Now that we have the sharded weights, we can then run inference with on demand weights loading. The maximum amount of memory that will be used at any one time is equal to the size of the largest shard plus the size of the batch. This allows inference on virtually any hardware, regardless of the model size.\n",
        "\n",
        "All we need now are two key components:\n",
        "\n",
        "1. `model_fn`: A function that returns an instance of our model. This function does not need to load weights, as the weights are initialized as empty. This means that our model will not consume any device memory, allowing us to initialize a model with billions of parameters even on an everyday laptop.\n",
        "\n",
        "    ```python\n",
        "    def model_fn() -> torch.nn.Module:\n",
        "        ...\n",
        "    ```\n",
        "\n",
        "2. `weights_loader_fn`: A function that takes the module name and a list of weight names as inputs, and returns a mapping from weight name to weight.\n",
        "\n",
        "    ```python\n",
        "    def weights_loader_fn(module_name: str, weight_names: list[str]\n",
        "    ) -> dict[str, torch.Tensor]:\n",
        "        ...\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HF_REPO_ID = \"philschmid/flan-t5-xxl-sharded-fp16\"\n",
        "CKPT_DIR = \"flan-t5-xxl-sharded-fp16\"\n",
        "\n",
        "# Prepare the `model_fn`. This is just a function that returns an instance of our model.\n",
        "# It is used to create an instance of our model that is initialized with empty weights and\n",
        "# thus doesn't consume device memory\n",
        "tokenizer = T5Tokenizer.from_pretrained(HF_REPO_ID)\n",
        "config = T5Config.from_pretrained(HF_REPO_ID)\n",
        "model_fn = lambda: T5ForConditionalGeneration(config)\n",
        "\n",
        "# Prepare the `weights_loader_fn`. This function performs the actual loading of weights\n",
        "# as well as making sure the weights have the correct data type and are on the right device.\n",
        "weights_loader_fn = ShardedWeightsLoader(\n",
        "    index_file_path=os.path.join(CKPT_DIR, \"pytorch_model.bin.index.json\"),\n",
        "    weights_dir=CKPT_DIR,\n",
        "    weights_mapping_key=\"weight_map\",\n",
        "    device=torch.device(\"cuda\"),\n",
        "    dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Initialize our odewel model. `enable_preloading` determines whether any extra weights returned\n",
        "# by the `weights_loader_fn` that are do not belong to the current module/sub-model will be loaded.\n",
        "odewel_model = init_on_demand_weights_model(\n",
        "    model_fn, weights_loader_fn, enable_preloading=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(tokenizer, model, prompt):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(input_ids, max_new_tokens=20)\n",
        "\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample from odewel model\n",
        "torch.manual_seed(0)\n",
        "PROMPT = \"translate English to German: How old are you?\"\n",
        "print(\"odewel Model:\", generate(tokenizer, odewel_model, PROMPT))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
