{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End Model Preparation and Inference ðŸš€\n",
    "\n",
    "This notebook demonstrates how to get your model ready for inference with `odewei` and perform efficient inference by loading weights only when they are needed. ðŸ”¥\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q git+https://github.com/Kinyugo/odewei.git\n",
    "%pip install -q joblib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    T5Config,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    ")\n",
    "\n",
    "from odewei.torch import ShardedWeightsLoader, init_on_demand_weights_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Checkpoints for On-Demand Loading\n",
    "\n",
    "ðŸ¥³ Get ready for limitless inference \n",
    "\n",
    "In this step, we'll load an existing model checkpoint and shard it, so that its weights can be loaded on-demand, instead of all at once. \n",
    "\n",
    "ðŸ’¡ Keep in mind, the sharding step has to be done on a device that can handle the full model size, as the full model state will be loaded. But, once the weights are sharded and saved, future inference tasks can run on any device that has at least the memory equal to the size of the largest shard.\n",
    "\n",
    "For simplicity, we'll use HuggingFace's implementation, but note that the size of the shard determines the minimum device memory that can be supported. An alternative is to shard the model in a layer-wise fashion, allowing for more dynamic control over which layers are pre-loaded together. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_REPO_ID = \"google/flan-t5-small\"\n",
    "CKPT_DIR = \"sharded-flan-t5-small\"\n",
    "\n",
    "# Load model checkpoints with the desired data type\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(HF_REPO_ID, torch_dtype=torch.float16)\n",
    "\n",
    "# Shard the model checkpoints, the size of the shard determines the mimimum GPU memory supported\n",
    "# in this case since the model is tiny we set a very small shard of 100MB but in a more realistic\n",
    "# use-case such as with the Flan-T5-XXL we would set something like 2000MB.\n",
    "model.save_pretrained(CKPT_DIR, max_shard_size=\"100MB\")\n",
    "\n",
    "\n",
    "# View the saved checkpoints\n",
    "print(os.listdir(CKPT_DIR))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference With On Demand Weights Loading\n",
    "\n",
    "Now that we have sharded the weights, we can then run inference with on demand weights loading. The maximum amount of memory that will be used at any one time is equal to the size of the largest shard plus the size of the batch. This allows inference on virtually any hardware, regardless of the model size.\n",
    "\n",
    "All we need now are two key components:\n",
    "\n",
    "1. `model_fn`: A function that returns an instance of our model. This function does not need to load weights, as the weights are initialized as empty. This means that our model will not consume any device memory, allowing us to initialize a model with billions of parameters even on an everyday laptop.\n",
    "\n",
    "    ```python\n",
    "    def model_fn() -> torch.nn.Module:\n",
    "        ...\n",
    "    ```\n",
    "\n",
    "2. `weights_loader_fn`: A function that takes the module name and a list of weight names as inputs, and returns a mapping from weight name to weight.\n",
    "\n",
    "    ```python\n",
    "    def weights_loader_fn(module_name: str, weight_names: list[str]\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        ...\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the `model_fn`. This is just a function that returns an instance of our model.\n",
    "# It is used to create an instance of our model that is initialized with empty weights and\n",
    "# thus doesn't consume device memory\n",
    "tokenizer = T5Tokenizer.from_pretrained(HF_REPO_ID)\n",
    "config = T5Config.from_pretrained(HF_REPO_ID)\n",
    "model_fn = lambda: T5ForConditionalGeneration(config)\n",
    "\n",
    "# Prepare the `weights_loader_fn`. This function performs the actual loading of weights\n",
    "# as well as making sure the weights have the correct data type and are on the right device.\n",
    "weights_loader_fn = ShardedWeightsLoader(\n",
    "    index_file_path=os.path.join(CKPT_DIR, \"pytorch_model.bin.index.json\"),\n",
    "    weights_dir=CKPT_DIR,\n",
    "    weights_mapping_key=\"weight_map\",\n",
    "    device=torch.device(\"cuda\"),\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Initialize our odewei model. `enable_preloading` determines whether any extra weights returned\n",
    "# by the `weights_loader_fn` that are do not belong to the current module/sub-model will be loaded.\n",
    "odewei_model = init_on_demand_weights_model(\n",
    "    model_fn, weights_loader_fn, enable_preloading=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"translate English to German: How old are you?\"\n",
    "\n",
    "\n",
    "def generate(tokenizer, model, prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids, max_new_tokens=20)\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the regular PyTorch model\n",
    "torch.manual_seed(0)\n",
    "print(\"Regular PyTorch Model:\", generate(tokenizer, model, PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from odewei model\n",
    "torch.manual_seed(0)\n",
    "print(\"odewei Model:\", generate(tokenizer, odewei_model, PROMPT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
